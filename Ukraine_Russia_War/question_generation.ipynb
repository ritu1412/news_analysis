{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, os, sys\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(chunk, client, model=\"gpt-4o-mini\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in news analysis and skilled at creating multiple-choice questions that help to differentiate between articles. \n",
    "I have some bodies of news articles and I need to generate unique, comprehensive multiple-choice questions based on these summaries.\n",
    "\n",
    "Your task is to generate questions that:\n",
    "- Help to differentiate between the articles.\n",
    "- Are common to all articles, not specific to any one article.\n",
    "- Cover various aspects such as focus, primary actors, geographical area, outcomes, public reaction, and specific mentions of organizations or events.\n",
    "- Are unique and not repeated in any other batch.\n",
    "- Include \"None of the above\" as one of the options for each question.\n",
    "\n",
    "Below is the format and example I need:\n",
    "\n",
    "1. What is the primary focus of the article?\n",
    "   - A. Military actions and attacks\n",
    "   - B. Political statements and negotiations\n",
    "   - C. Humanitarian impact and casualties\n",
    "   - D. Protests and public reactions\n",
    "   - E. None of the above\n",
    "\n",
    "\n",
    "Please generate as many unique and comprehensive questions as possible based on the given article bodies below. \n",
    "Make sure each question is designed to highlight differences between the articles.\n",
    "\n",
    "Summaries: {chunk}\n",
    "\n",
    "Generate as many as you can unique questions based on the above summaries.\n",
    "\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    except Exception as e:  # if the model fails to return a response\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, error from GPT.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(input_file, output_file, client, model):\n",
    "    # Read the article summaries from the input JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(articles)\n",
    "    \n",
    "    # Extract summaries from the articles\n",
    "    article_summaries = [article['body'] for article in articles]\n",
    "    num_summaries = len(article_summaries)\n",
    "    batch_size = 100\n",
    "    \n",
    "    responses = []\n",
    "\n",
    "    # Process the article summaries in batches of 100\n",
    "    for i in range(0, num_summaries, batch_size):\n",
    "        batch = article_summaries[i:i+batch_size]\n",
    "        chunk = json.dumps(batch)  # Convert the batch to a JSON string\n",
    "\n",
    "        questions = generate_questions(chunk, client, model=model)\n",
    "        responses.append({\n",
    "            \"batch_start\": i,\n",
    "            \"batch_end\": min(i+batch_size-1, num_summaries-1),\n",
    "            \"questions\": questions\n",
    "        })\n",
    "    \n",
    "    # Write the generated questions to the output JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'gpt_generated_articles.json'\n",
    "output_file = 'output_questions_synthetic_data.json'\n",
    "process_batches(input_file, output_file, client, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(batch_questions):\n",
    "    questions = {}\n",
    "    q_count = 1\n",
    "    for batch in batch_questions:\n",
    "        batch_qs = batch[\"questions\"].split('\\n\\n')\n",
    "        for q in batch_qs:\n",
    "            match = re.match(r\"\\d+\\. (.+?)\\n\\s+- A\\. (.+?)\\n\\s+- B\\. (.+?)\\n\\s+- C\\. (.+?)\\n\\s+- D\\. (.+?)\\n\\s+- E\\. (.+?)$\", q.strip(), re.DOTALL)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                choices = {\n",
    "                    \"A\": match.group(2).strip(),\n",
    "                    \"B\": match.group(3).strip(),\n",
    "                    \"C\": match.group(4).strip(),\n",
    "                    \"D\": match.group(5).strip(),\n",
    "                    \"E\": match.group(6).strip()\n",
    "                }\n",
    "                if question not in questions:\n",
    "                    questions[question] = choices\n",
    "    return questions\n",
    "\n",
    "def convert_to_json2_format(unique_questions):\n",
    "    formatted_questions = {}\n",
    "    q_num = 1\n",
    "    for question, choices in unique_questions.items():\n",
    "        formatted_questions[f\"Q{q_num}\"] = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices\n",
    "        }\n",
    "        q_num += 1\n",
    "    return formatted_questions\n",
    "\n",
    "def process_json(input_json):\n",
    "    unique_questions = extract_questions(input_json)\n",
    "    return convert_to_json2_format(unique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON 1\n",
    "with open('output_questions_synthetic_data.json', 'r') as f:\n",
    "    json1 = json.load(f)\n",
    "\n",
    "# Process JSON 1 to get JSON 2 format\n",
    "json2_format = process_json(json1)\n",
    "\n",
    "# Save the result to a new JSON file\n",
    "with open('formated_output_questions_synthetic_data.json', 'w') as f:\n",
    "    json.dump(json2_format, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
