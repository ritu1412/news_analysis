{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, os, sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the clusters to find the narrative differences\n",
    "def narrative_difference(chunk, client, model=\"gpt-4o\"):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in news analysis and skilled at capturing narrative difference.\n",
    "    I have some news articles in English and Hindi and I need to you to help me identify the narrative differences between them.\n",
    "    Please provide a summary of the narrative differences between the English and Hindi articles.\n",
    "    Along with your response, please provide a few examples from the articles to support your analysis.\n",
    "    {chunk}\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    except Exception as e:  # if the model fails to return a response\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, error from GPT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clusters(input_file, output_file, client, model, batch_size=25):\n",
    "    # Read the articles from the input JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(articles)\n",
    "    \n",
    "    # Convert the list of articles to a DataFrame\n",
    "    articles_df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Check if 'cluster' column exists in the DataFrame\n",
    "    if 'cluster' not in articles_df.columns:\n",
    "        raise ValueError(\"The input JSON file does not contain 'cluster' labels for the articles.\")\n",
    "    \n",
    "    responses = []\n",
    "\n",
    "    # Group articles by their cluster labels\n",
    "    grouped_articles = articles_df.groupby('cluster')\n",
    "\n",
    "    # Process each cluster separately\n",
    "    for cluster, group in grouped_articles:\n",
    "        article_bodies = group['body'].tolist()\n",
    "        \n",
    "        # Split articles into batches\n",
    "        for i in range(0, len(article_bodies), batch_size):\n",
    "            batch = article_bodies[i:i+batch_size]\n",
    "            chunk = json.dumps(batch)  # Convert the batch to a JSON string\n",
    "\n",
    "            questions = narrative_difference(chunk, client, model=model)\n",
    "            responses.append({\n",
    "                \"cluster\": int(cluster),\n",
    "                \"part\": i // batch_size + 1,\n",
    "                \"narrative_difference\": questions\n",
    "            })\n",
    "    \n",
    "    # Write the generated questions to the output JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the json file to the desired length\n",
    "def clip_json(input_file, output_file, length):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Clip the data to the desired length\n",
    "    clipped_data = data[:length]\n",
    "    \n",
    "    # Write the clipped data to the output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(clipped_data, f, indent=4)\n",
    "\n",
    "clip_json(\"data/summary_cluster_all.json\",\"summary_eng_hin.json\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 137853 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "process_clusters(\"summary_eng_hin.json\", \"Ukraine_Russia_narrative_english_prompt.json\", client, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to formatted_articles.csv\n"
     ]
    }
   ],
   "source": [
    "#read the json file\n",
    "with open(\"Ukraine_Russia_narrative_by_cluster.json\") as f:\n",
    "    data = json.load(f)\n",
    "# Convert JSON data to DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "output_file = 'formatted_articles.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
